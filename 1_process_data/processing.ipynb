{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data path and read it\n",
    "path_to_data = 'C:/myDocuments/projects/instant-noodles/0_webscrape_data/data/instant_noodles.csv'\n",
    "\n",
    "raw_data =  pl.read_csv(path_to_data, encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data from string to numeric\n",
    "extracted_data = raw_data.with_columns(\n",
    "        pl.col(\"price\").str.extract(r\"(\\d+\\..*\\d*)\", 1).alias(\"price\").cast(pl.Float32), # price in GBP\n",
    "        pl.col(\"weight\").str.extract(r\"(\\d+)g.*\", 1).alias(\"weight\").cast(pl.Float32), # weight in grams\n",
    "        pl.col(\"weight\").str.extract(r\"\\d+g.*,.*(\\d+).*\", 1).alias(\"servings\").cast(pl.Int32).fill_null(1), # Servings if multipack\n",
    "    )\n",
    "\n",
    "# Add adjustment to price and weight according to the serving size\n",
    "processed_data = extracted_data.with_columns(\n",
    "    (pl.col(\"price\")/ pl.col(\"servings\")).round(2).alias(\"price\"),\n",
    "    (pl.col(\"weight\")/ pl.col(\"servings\")).round(0).alias(\"weight\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each maker to a country to help the transformer model\n",
    "mapping = {\n",
    "    'Japan': ['Sanyo', 'Nissin Foods', 'Itsuki', 'Hikari Miso', 'Yamadai', 'Higashi Foods', 'Higashimaru', 'Yamamoto Seifun', 'Otafuku', 'Maruchan', 'Daikoku Foods'],\n",
    "    'Singapore' : ['Myojo'],\n",
    "    'South Korea' : ['Nong Shim','Samyang Foods','GARAK','Young Poong Foods'],\n",
    "    'Vietnam' : ['Acecook']\n",
    "}\n",
    "\n",
    "assistant_prompt = 'Here are some useful information that might aid you in getting the region correct. '\n",
    "\n",
    "# Create assistant prompt\n",
    "for country in mapping:\n",
    "    assistant_prompt += 'The following maker(s) are from ' + country + ': '\n",
    "    for maker in mapping[country]:\n",
    "        assistant_prompt += maker + ', '\n",
    "    assistant_prompt = assistant_prompt[:-2] + '. '\n",
    "\n",
    "# Additional assistance - region\n",
    "assistant_prompt = assistant_prompt + 'Another useful tip, when product name has ramen in it, its region is usually Japan. Do note that when you see brackets in the product name, e.g. \"(USA)\", this means the noodle is distributed in that country, not necessarily meaning the instant noodles is from that region. Besides that, if region is Korea, please update these to South Korea. '\n",
    "\n",
    "# Additional assistance - Spicy, Soupy, Flavour\n",
    "assistant_prompt = assistant_prompt + 'Besides, udon and ramen noodles usually means the noodles originate from Japan. When deciding whether the noodles is spicy, soupy, or what type of base ingredietn it has. Please refer to the product name and description of the product. It should help you make a good decision. If undecided, when ramen is seen in the product name or description, it is usually one of these three: Vegetable, Pork or Chicken. Sesame oil flavoured instant noodles are considered as having Vegetable as the base ingredient. '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Number of rows of data\n",
    "nRow = processed_data.shape[0]\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(nRow):\n",
    "\n",
    "    # Extract data from row\n",
    "    product = str(processed_data['product'].to_list()[i])\n",
    "    maker = str(processed_data['maker'].to_list()[i])\n",
    "    description = str(processed_data['description'].to_list()[i])\n",
    "\n",
    "    # Create user prompt\n",
    "    user_prompt = 'Name of instant noodles: ' + product + '. Description: ' + description + '. Maker: ' + maker + '.'\n",
    "\n",
    "    instructions_prompt = 'please give your answer in the following format: [*Which region is the noodle inspired from*, *Spicy - Yes or No*, *Soupy - Yes or No*, *Which describes the ingredient base best, please select one of the options - Seafood/Chicken/Beef/Lamb/Pork/Vegetable/Vegan*]. For example, an output could look like [Japan, No, Yes, Chicken].'\n",
    "    # Use the new `openai.ChatCompletion.create` method\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": \"You are a helping to identify some characteristics of different kinds of instant noodles. Please try to answer the questions as correctly as possible based on the name, description, and maker of the instant noodles provided to you. For each product, \"+ instructions_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Log responses from gpt model\n",
    "    all_outputs.append(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs from chat GPT\n",
    "gpt_output = processed_data.with_columns(\n",
    "        pl.Series('gpt_outputs', all_outputs),\n",
    "    )\n",
    "\n",
    "# Add each category into a new column \n",
    "output_data = gpt_output.with_columns(\n",
    "        pl.col('maker').str.replace_all(\" \", \"\").str.to_lowercase(), # Set maker to lowercase and remove spaces\n",
    "        pl.col('gpt_outputs').str.extract(r\"\\[(.*), .*, .*, .*\\]\", 1).str.replace_all(\" \", \"\").str.to_lowercase().alias(\"region\"),\n",
    "        pl.col('gpt_outputs').str.extract(r\"\\[.*, (.*), .*, .*\\]\", 1).str.replace_all(\" \", \"\").str.to_lowercase().alias(\"spicy\"),\n",
    "        pl.col('gpt_outputs').str.extract(r\"\\[.*, .*, (.*), .*\\]\", 1).str.replace_all(\" \", \"\").str.to_lowercase().alias(\"soupy\"),\n",
    "        pl.col('gpt_outputs').str.extract(r\"\\[.*, .*, .*, (.*)\\]\", 1).str.replace_all(\" \", \"\").str.to_lowercase().alias(\"base\")\n",
    "    )\n",
    "\n",
    "# Create a version of the data that only contains columns we are interested in\n",
    "final_data = output_data.select(\n",
    "    ['product', 'price', 'weight', 'maker', 'region', 'spicy', 'soupy', 'base']\n",
    ")\n",
    "\n",
    "# Write both final data and the complete output data\n",
    "final_data.write_csv('data/processed_data.csv')\n",
    "output_data.write_csv('data/all_output_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
